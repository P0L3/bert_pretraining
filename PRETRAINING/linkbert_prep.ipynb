{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1414ab8",
   "metadata": {},
   "source": [
    "A notebook to explore availability of data for LinkBERT style pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789d671",
   "metadata": {},
   "source": [
    "## 0 Data, LIbraires and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40350ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from semanticscholar import SemanticScholar\n",
    "from tqdm.auto import tqdm\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from crossref.restful import Works, CrossrefAPIError \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0f02ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_doi(doi_string):\n",
    "    \"\"\"\n",
    "    Clean and standardize a DOI (Digital Object Identifier) string.\n",
    "    \"\"\"\n",
    "    if isinstance(doi_string, list):\n",
    "        doi_string = doi_string[0] if doi_string else None\n",
    "    if not isinstance(doi_string, str):\n",
    "        return None\n",
    "    doi_string = doi_string.lstrip('/')\n",
    "    if 'doi.org/' in doi_string:\n",
    "        doi_string = doi_string.split('doi.org/')[-1]\n",
    "    return doi_string.strip()\n",
    "\n",
    "def fix_title(title):\n",
    "    \"\"\"\n",
    "    Normalizes a paper title string, using a simple rule to detect spaced-out text.\n",
    "    Args:\n",
    "        title (str or any): The input title to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized title string. Returns an empty string if input is not a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "\n",
    "    if \"  \" in title:\n",
    "        placeholder = \"@@@\"\n",
    "        \n",
    "        title_fixed = re.sub(r'\\s{2,}', placeholder, title)\n",
    "        \n",
    "        title_fixed = title_fixed.replace(' ', '')\n",
    "        \n",
    "        title = title_fixed.replace(placeholder, ' ')\n",
    "        \n",
    "    title = title.lower()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = \" \".join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def get_papers_s2_batch(sch, doi_chunk, fields):\n",
    "    \"\"\"\n",
    "    Takes one shot at fetching a batch from Semantic Scholar.\n",
    "    On any failure, it returns None, indicating the batch failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # A single attempt to fetch the batch.\n",
    "        return sch.get_papers(doi_chunk, fields=fields)\n",
    "    except Exception:\n",
    "        # Catch TypeError, network errors, etc. If anything goes wrong,\n",
    "        # signal failure by returning None.\n",
    "        return None\n",
    "\n",
    "def get_papers_robust(sch, doi_list, fields, failed_dois_list):\n",
    "    if not doi_list: return []\n",
    "    try:\n",
    "        return sch.get_papers(doi_list, fields=fields)\n",
    "    except TypeError:\n",
    "        if len(doi_list) == 1:\n",
    "            failed_dois_list.append((doi_list[0], 'TypeError'))\n",
    "            return []\n",
    "        tqdm.write(f\"S2 Batch of {len(doi_list)} failed. Splitting...\")\n",
    "        mid = len(doi_list) // 2\n",
    "        return get_papers_robust(sch, doi_list[:mid], fields, failed_dois_list) + \\\n",
    "               get_papers_robust(sch, doi_list[mid:], fields, failed_dois_list)\n",
    "    except Exception as e:\n",
    "        for doi in doi_list: failed_dois_list.append((doi, str(e)))\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b40ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_pickle(\"DATASET/ED4RE_2503/ED4RE_2603.pickle\")\n",
    "df_full[\"backtrace_index\"] = range(len(df_full))\n",
    "df_full[\"DOI_processed\"] = df_full[\"DOI\"].apply(preprocess_doi)\n",
    "df_full[\"Title_processed\"] = df_full[\"Title\"].apply(fix_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16b09e",
   "metadata": {},
   "source": [
    "## 1 DOI Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d15979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a3de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master DOI-to-info map from the corpus...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0028188228607177734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Mapping DOIs",
       "rate": null,
       "total": 10000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976503c4baae462caafb15eed2d34286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping DOIs:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a lookup map and a query list with 9786 unique DOIs.\n",
      "\n",
      "--- Pass 1: Querying Semantic Scholar in 196 batches... ---\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002373218536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "S2 Fetching",
       "rate": null,
       "total": 196,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7542c1a2c7904840922514fb743959f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "S2 Fetching:   0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p0l3/.local/lib/python3.10/site-packages/semanticscholar/AsyncSemanticScholar.py:193: UserWarning: IDs not found: ['10.1029/200', '10.1073/pnas.98.1.283']\n",
      "  warnings.warn(f\"IDs not found: {not_found_ids}\")\n",
      "/home/p0l3/.local/lib/python3.10/site-packages/semanticscholar/AsyncSemanticScholar.py:193: UserWarning: IDs not found: ['10.1029/2002JD002932']\n",
      "  warnings.warn(f\"IDs not found: {not_found_ids}\")\n",
      "/home/p0l3/.local/lib/python3.10/site-packages/semanticscholar/AsyncSemanticScholar.py:193: UserWarning: IDs not found: ['10.3390/w13172426', '10.1073/pnas.2009911117']\n",
      "  warnings.warn(f\"IDs not found: {not_found_ids}\")\n"
     ]
    }
   ],
   "source": [
    "# 00:03:00 for 500 examples\n",
    "sch = SemanticScholar()\n",
    "works = Works() \n",
    "\n",
    "# Create the master lookup map for node attributes\n",
    "print(\"Creating master DOI-to-info map from the corpus...\")\n",
    "doi_to_info_map = {}\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Mapping DOIs\"):\n",
    "    clean_doi = row['DOI_processed']\n",
    "    if clean_doi:\n",
    "        doi_to_info_map[clean_doi] = {\n",
    "            'index': row[\"backtrace_index\"],\n",
    "            'title': row['Title_processed']\n",
    "        }\n",
    "\n",
    "corpus_doi_set = set(doi_to_info_map.keys())\n",
    "all_dois_to_query = list(corpus_doi_set)\n",
    "print(f\"Created a lookup map and a query list with {len(all_dois_to_query)} unique DOIs.\")\n",
    "\n",
    "\n",
    "fields_s2 = ['externalIds', 'citations.externalIds']\n",
    "chunk_size = 50\n",
    "doi_chunks = [all_dois_to_query[i:i + chunk_size] for i in range(0, len(all_dois_to_query), chunk_size)]\n",
    "s2_papers = []\n",
    "dois_for_crossref = []\n",
    "\n",
    "print(f\"\\n--- Pass 1: Querying Semantic Scholar in {len(doi_chunks)} batches... ---\")\n",
    "for chunk in tqdm(doi_chunks, desc=\"S2 Fetching\"):\n",
    "    \n",
    "    results = get_papers_s2_batch(sch, chunk, fields_s2)\n",
    "    \n",
    "    if results is None:\n",
    "        # The entire batch failed. Delegate the whole chunk to Crossref.\n",
    "        dois_for_crossref.extend(chunk)\n",
    "    else:\n",
    "        # The batch succeeded. Process the results.\n",
    "        s2_papers.extend(results)\n",
    "        \n",
    "        # Still need to find which DOIs were \"Not Found\" by the API.\n",
    "        found_dois = {p.externalIds['DOI'] for p in results if p and p.externalIds and 'DOI' in p.externalIds}\n",
    "        not_found_dois = set(chunk) - found_dois\n",
    "        dois_for_crossref.extend(list(not_found_dois))\n",
    "        \n",
    "    time.sleep(3)\n",
    "    \n",
    "\n",
    "# Process S2 results\n",
    "s2_links = []\n",
    "for paper in tqdm(s2_papers, desc=\"Building S2 Links\"):\n",
    "    if paper and paper.externalIds and (cited_doi := paper.externalIds.get('DOI')):\n",
    "        if paper.citations:\n",
    "            for citing_paper in paper.citations:\n",
    "                if citing_paper and citing_paper.externalIds and (citing_doi := citing_paper.externalIds.get('DOI')):\n",
    "                    if citing_doi in corpus_doi_set:\n",
    "                        s2_links.append({'source_doi': citing_doi, 'target_doi': cited_doi})\n",
    "\n",
    "print(f\"Semantic Scholar Pass: Found {len(s2_links)} links. Failed on {len(dois_for_crossref)} DOIs.\")\n",
    "\n",
    "crossref_links = []\n",
    "\n",
    "print(f\"\\n--- Pass 2: Querying Crossref for {len(dois_for_crossref)} failed DOIs... ---\")\n",
    "for source_doi in tqdm(dois_for_crossref, desc=\"Crossref Fallback\"):\n",
    "    try:\n",
    "        work = works.doi(source_doi)\n",
    "        if work and 'reference' in work:\n",
    "            for ref in work['reference']:\n",
    "                if 'DOI' in ref:\n",
    "                    target_doi = preprocess_doi(ref['DOI'])\n",
    "                    if target_doi in corpus_doi_set:\n",
    "                        crossref_links.append({'source_doi': source_doi, 'target_doi': target_doi})\n",
    "    except CrossrefAPIError as e:\n",
    "        # Specifically handles 404 Not Found from Crossref\n",
    "        pass \n",
    "    except Exception:\n",
    "        # Catches other potential errors (network, etc.)\n",
    "        pass\n",
    "    time.sleep(0.1) # Be gentle with the Crossref API\n",
    "\n",
    "print(f\"Crossref Pass: Found {len(crossref_links)} additional links.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Combining results and building final graph... ---\")\n",
    "# Create DataFrames and combine them\n",
    "s2_links_df = pd.DataFrame(s2_links)\n",
    "crossref_links_df = pd.DataFrame(crossref_links)\n",
    "final_links_df = pd.concat([s2_links_df, crossref_links_df]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Total unique links from all sources: {len(final_links_df)}\")\n",
    "\n",
    "# Explicitly build the graph to include attributes\n",
    "G = nx.DiGraph()\n",
    "for doi, info in doi_to_info_map.items():\n",
    "    G.add_node(doi, index=info['index'], title=info['title'])\n",
    "\n",
    "for _, row in final_links_df.iterrows():\n",
    "    if G.has_node(row['source_doi']) and G.has_node(row['target_doi']):\n",
    "        G.add_edge(row['source_doi'], row['target_doi'])\n",
    "\n",
    "print(f\"\\nFinal graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "\n",
    "# Verify and save\n",
    "nx.write_graphml(G, F\"CG_N{len(G.nodes)}_E{len(G.edges)}.graphml\")\n",
    "print(\"Graph saved to hybrid_citation_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dc90aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R a p i d   g r o w t h   o f   t h e   U S   w i l d l a n d - u r b a n   i n t e r f a c e   r a i s e s   w i l d f i r e   r i s k\n",
      "rapid growth of the us wildlandurban interface raises wildfire risk\n",
      "10.1073/pnas.1718850115\n"
     ]
    }
   ],
   "source": [
    "ID = 90506\n",
    "print(df_full.iloc[ID][\"Title\"])\n",
    "print(df_full.iloc[ID][\"Title_processed\"])\n",
    "print(df_full.iloc[ID][\"DOI_processed\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb80c2",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e95ca",
   "metadata": {},
   "source": [
    "## 1 Initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc7649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Analysis of Reference Column Structure\n",
      "============================================================\n",
      "Total documents with potential references: 171,880\n",
      "Documents with references as a list (good format): 162,726 (94.67%)\n",
      "Documents with references as a single string (needs parsing): 9,154 (5.33%)\n",
      "\n",
      "\n",
      "--- For List-Formatted References Only ---\n",
      "count    162726.000000\n",
      "mean         52.317902\n",
      "std         583.275877\n",
      "min           0.000000\n",
      "25%          33.000000\n",
      "50%          45.000000\n",
      "75%          60.000000\n",
      "max      169759.000000\n",
      "Name: num_references, dtype: float64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 1: Calculate the number of references for each document ---\n",
    "def get_reference_count(ref_entry):\n",
    "    if isinstance(ref_entry, list):\n",
    "        return len(ref_entry)\n",
    "    elif ref_entry == \"no_references\":\n",
    "        return 0\n",
    "    else: # This handles strings and other non-list types\n",
    "        return 1\n",
    "\n",
    "# Apply this function to create a new column with the counts\n",
    "df['num_references'] = df['References'].apply(get_reference_count)\n",
    "\n",
    "\n",
    "# --- Step 2: Get high-level statistics ---\n",
    "\n",
    "# First, let's count how many are neatly structured vs. single blobs\n",
    "is_list_mask = df['References'].apply(lambda x: isinstance(x, list))\n",
    "list_count = is_list_mask.sum()\n",
    "string_blob_count = len(df) - list_count\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Analysis of Reference Column Structure\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total documents with potential references: {len(df):,}\")\n",
    "print(f\"Documents with references as a list (good format): {list_count:,} ({list_count/len(df):.2%})\")\n",
    "print(f\"Documents with references as a single string (needs parsing): {string_blob_count:,} ({string_blob_count/len(df):.2%})\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# # Now, let's get descriptive statistics on the 'num_references' column\n",
    "# print(\"=\"*60)\n",
    "# print(\"Descriptive Statistics for Number of References per Document\")\n",
    "# print(\"=\"*60)\n",
    "# # The describe() output will be heavily influenced by the single-string blobs (value=1)\n",
    "# # so we'll show stats for both all data and just the list-formatted data.\n",
    "# print(\"--- Overall (including single-string blobs as 1) ---\")\n",
    "# print(df['num_references'].describe())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# Filter for only the documents that had a list to get a cleaner distribution\n",
    "df_lists_only = df[is_list_mask]\n",
    "print(\"--- For List-Formatted References Only ---\")\n",
    "print(df_lists_only['num_references'].describe())\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a163e1",
   "metadata": {},
   "source": [
    "**Observation 1**:\n",
    "\n",
    "Total documents with potential references: 171,880\n",
    "Documents with references as a list (good format): 162,726 (94.67%)\n",
    "Documents with references as a single string (needs parsing): 9,154 (5.33%)\n",
    "\n",
    "count    162726\n",
    "mean         52.317902\n",
    "std         583.275877\n",
    "min           0\n",
    "25%          33\n",
    "50%          45\n",
    "75%          60\n",
    "max      169759\n",
    "\n",
    "75% of references are lists that contain up to 60 references, this seems like a reasonable maximum number? Let's look at a random sample to feel the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df_lists_only[df_lists_only.num_references <= 10].sample(100)[\"Title\"].to_list()\n",
    "\n",
    "def fix_title(title):\n",
    "    \"\"\"\n",
    "    Normalizes a paper title string, using a simple rule to detect spaced-out text.\n",
    "\n",
    "    The logic is:\n",
    "    1. Check if the title contains a double-space (\"  \").\n",
    "    2. IF IT DOES: Assume it's \"spaced-out\" text (e.g., \"T i t l e   W o r d\").\n",
    "       - Mark the real word breaks (the double-spaces) with a placeholder.\n",
    "       - Remove all single spaces (the junk between letters).\n",
    "       - Restore the real word breaks.\n",
    "    3. IF IT DOES NOT: Treat it as a normal title.\n",
    "    4. Finally, apply standard cleaning (lowercase, remove punctuation, etc.) to all titles.\n",
    "\n",
    "    Args:\n",
    "        title (str or any): The input title to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized title string. Returns an empty string if input is not a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "\n",
    "    if \"  \" in title:\n",
    "        placeholder = \"@@@\"\n",
    "        \n",
    "        title_fixed = re.sub(r'\\s{2,}', placeholder, title)\n",
    "        \n",
    "        title_fixed = title_fixed.replace(' ', '')\n",
    "        \n",
    "        title = title_fixed.replace(placeholder, ' ')\n",
    "        \n",
    "    title = title.lower()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = \" \".join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "for title in a:\n",
    "    print(title)\n",
    "    print(fix_title(title))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b2bee",
   "metadata": {},
   "source": [
    "This function seems to be working for titles, next, reference titles should be tackled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_from_reference(ref_string):\n",
    "    \"\"\"\n",
    "    Extracts the most likely title from a scientific reference string.\n",
    "\n",
    "    This uses a simple but robust heuristic:\n",
    "    1. Removes bracketed metadata (e.g., [CrossRef]).\n",
    "    2. Splits the reference by periods ('.').\n",
    "    3. Assumes the longest resulting segment is the title.\n",
    "\n",
    "    Args:\n",
    "        ref_string (str): The reference string to parse.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted title, or None if no suitable title is found.\n",
    "    \"\"\"\n",
    "    if not isinstance(ref_string, str):\n",
    "        return None\n",
    "\n",
    "    # 1. Pre-clean: Remove bracketed metadata like [Google Scholar] [CrossRef]\n",
    "    cleaned_ref = re.sub(r'\\[.*?\\]', '', ref_string).strip()\n",
    "    \n",
    "    # 2. Split the string into major parts using the period as a delimiter\n",
    "    parts = cleaned_ref.split('.')\n",
    "    \n",
    "    # 3. Filter out empty or very short parts that can't be titles\n",
    "    # A title usually has some substance. We also strip whitespace from each part.\n",
    "    potential_titles = [part.strip() for part in parts if len(part.strip()) > 10]\n",
    "    \n",
    "    # If, after filtering, there are no candidates, we can't find a title\n",
    "    if not potential_titles:\n",
    "        return None\n",
    "        \n",
    "    # 4. Select the longest part from the candidates\n",
    "    # The title is almost always the longest meaningful segment of a reference.\n",
    "    title = max(potential_titles, key=len)\n",
    "    \n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e35435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref in df_lists_only[df_lists_only.num_references <= 100].sample(1)[\"References\"].to_list()[0]:\n",
    "    print(ref)\n",
    "    print(extract_title_from_reference(ref))\n",
    "    print(fix_title(extract_title_from_reference(ref)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24276e",
   "metadata": {},
   "source": [
    "This setup seems to work good for references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756626f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(date_string):\n",
    "    \"\"\"\n",
    "    Extracts the most likely publication year from a messy date string.\n",
    "\n",
    "    This function tries several strategies in order:\n",
    "    1. Looks for a year in a \"Published: ...\" line.\n",
    "    2. If not found, it searches for any four-digit number starting with 19 or 20.\n",
    "    3. Returns the last year found in the string, as 'Published' is usually last.\n",
    "\n",
    "    Args:\n",
    "        date_string (str): The messy string from the 'Date' column.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted four-digit year as an integer, or None if no\n",
    "                     plausible year can be found.\n",
    "    \"\"\"\n",
    "    if not isinstance(date_string, str):\n",
    "        return None\n",
    "\n",
    "    published_match = re.search(r'Published: .*?(\\b(19|20)\\d{2}\\b)', date_string)\n",
    "    if published_match:\n",
    "        return int(published_match.group(1))\n",
    "\n",
    "    all_years = re.findall(r'\\b(?:19|20)\\d{2}\\b', date_string)\n",
    "    \n",
    "    if all_years:\n",
    "        return int(all_years[-1])\n",
    "\n",
    "    # If no year could be found, return None\n",
    "    return \"no_date\"\n",
    "\n",
    "for date in df_lists_only.Date.sample(100).tolist():\n",
    "    print(date)\n",
    "    print(extract_year(date))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d9f86",
   "metadata": {},
   "source": [
    "## 2 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd69290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm.auto import tqdm # For a nice progress bar!\n",
    "\n",
    "\n",
    "def fix_title(title):\n",
    "    \"\"\"\n",
    "    Normalizes a paper title string, using a simple rule to detect spaced-out text.\n",
    "    Args:\n",
    "        title (str or any): The input title to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized title string. Returns an empty string if input is not a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "\n",
    "    if \"  \" in title:\n",
    "        placeholder = \"@@@\"\n",
    "        \n",
    "        title_fixed = re.sub(r'\\s{2,}', placeholder, title)\n",
    "        \n",
    "        title_fixed = title_fixed.replace(' ', '')\n",
    "        \n",
    "        title = title_fixed.replace(placeholder, ' ')\n",
    "        \n",
    "    title = title.lower()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = \" \".join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "def extract_title_from_reference(ref_string):\n",
    "    \"\"\"\n",
    "    Extracts the most likely title from a scientific reference string.\n",
    "    Args:\n",
    "        ref_string (str): The reference string to parse.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted title, or None if no suitable title is found.\n",
    "    \"\"\"\n",
    "    if not isinstance(ref_string, str):\n",
    "        return None\n",
    "\n",
    "    # 1. Pre-clean: Remove bracketed metadata like [Google Scholar] [CrossRef]\n",
    "    cleaned_ref = re.sub(r'\\[.*?\\]', '', ref_string).strip()\n",
    "    \n",
    "    # 2. Split the string into major parts using the period as a delimiter\n",
    "    parts = cleaned_ref.split('.')\n",
    "    \n",
    "    # 3. Filter out empty or very short parts that can't be titles\n",
    "    # A title usually has some substance. We also strip whitespace from each part.\n",
    "    potential_titles = [part.strip() for part in parts if len(part.strip()) > 10]\n",
    "    \n",
    "    # If, after filtering, there are no candidates, we can't find a title\n",
    "    if not potential_titles:\n",
    "        return None\n",
    "        \n",
    "    # 4. Select the longest part from the candidates\n",
    "    # The title is almost always the longest meaningful segment of a reference.\n",
    "    title = max(potential_titles, key=len)\n",
    "    \n",
    "    return title\n",
    "\n",
    "def extract_year(date_string):\n",
    "    \"\"\"\n",
    "    Extracts the most likely publication year from a messy date string.\n",
    "    Args:\n",
    "        date_string (str): The messy string from the 'Date' column.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted four-digit year as an integer, or None if no\n",
    "                     plausible year can be found.\n",
    "    \"\"\"\n",
    "    if not isinstance(date_string, str):\n",
    "        return None\n",
    "\n",
    "    published_match = re.search(r'Published: .*?(\\b(19|20)\\d{2}\\b)', date_string)\n",
    "    if published_match:\n",
    "        return int(published_match.group(1))\n",
    "\n",
    "    all_years = re.findall(r'\\b(?:19|20)\\d{2}\\b', date_string)\n",
    "    \n",
    "    if all_years:\n",
    "        return int(all_years[-1])\n",
    "\n",
    "    # If no year could be found, return None\n",
    "    return \"no_date\"\n",
    "\n",
    "def find_best_match_for_reference_v2(ref_title, corpus_df, titles_map, ref_year=None, threshold=85, year_tol=5):\n",
    "    if not ref_title: return None, None, None\n",
    "    if ref_title in titles_map: return ref_title, titles_map[ref_title], 100\n",
    "    \n",
    "    candidate_corpus = corpus_df\n",
    "    if ref_year is not None and 'year' in corpus_df.columns and pd.api.types.is_numeric_dtype(corpus_df['year']):\n",
    "        filtered_view = corpus_df[corpus_df['year'].between(ref_year - year_tol, ref_year + year_tol)]\n",
    "        if not filtered_view.empty: candidate_corpus = filtered_view\n",
    "            \n",
    "    candidate_titles = list(candidate_corpus['normalized_title'])\n",
    "    candidate_indices = list(candidate_corpus.index)\n",
    "    if not candidate_titles: return None, None, None\n",
    "\n",
    "    best_match = process.extractOne(ref_title, candidate_titles, scorer=fuzz.WRatio, score_cutoff=threshold)\n",
    "\n",
    "    if best_match:\n",
    "        matched_title_str, score, original_list_index = best_match\n",
    "        corpus_idx = candidate_indices[original_list_index]\n",
    "        return matched_title_str, corpus_idx, score\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "df = df_lists_only[df_lists_only.num_references <= 100] # Use your actual DataFrame here\n",
    "\n",
    "print(\"Preparing the corpus...\")\n",
    "# Use tqdm to see progress on large dataframes\n",
    "tqdm.pandas(desc=\"Normalizing Titles\")\n",
    "df['normalized_title'] = df['Title'].progress_apply(fix_title)\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting Years\")\n",
    "df['year'] = df['Date'].progress_apply(extract_year)\n",
    "\n",
    "# Create the fast lookup map for exact matches\n",
    "print(\"Creating title-to-index map...\")\n",
    "corpus_titles_map = {title: idx for idx, title in df['normalized_title'].items() if title}\n",
    "\n",
    "\n",
    "# --- Step 3: The Main Linking Loop ---\n",
    "\n",
    "print(\"\\nStarting the reference linking process...\")\n",
    "citation_links = []\n",
    "# We iterate through each row of the DataFrame\n",
    "for source_index, row in tqdm(df.iterrows(), total=len(df), desc=\"Linking Documents\"):\n",
    "    references = row['References']\n",
    "    source_title = row['Title']\n",
    "    \n",
    "    # Ensure references is a list of strings\n",
    "    if not isinstance(references, list):\n",
    "        continue\n",
    "\n",
    "    for ref_string in references:\n",
    "        # Extract and normalize title from the reference string\n",
    "        raw_ref_title = extract_title_from_reference(ref_string)\n",
    "        norm_ref_title = fix_title(raw_ref_title)\n",
    "        \n",
    "        # Extract year from the reference string\n",
    "        ref_year = extract_year(ref_string)\n",
    "\n",
    "        # Find the best match in the entire corpus\n",
    "        matched_title, target_index, score = find_best_match_for_reference_v2(\n",
    "            norm_ref_title,\n",
    "            df,\n",
    "            corpus_titles_map,\n",
    "            ref_year=ref_year,\n",
    "            threshold=85, # You can tune this!\n",
    "            year_tol=2\n",
    "        )\n",
    "\n",
    "        # If a good match was found, store it\n",
    "        if target_index is not None:\n",
    "            citation_links.append({\n",
    "                'source_index': source_index,\n",
    "                'source_title': source_title,\n",
    "                'reference_string': ref_string,\n",
    "                'target_index': target_index,\n",
    "                'matched_corpus_title': df.loc[target_index, 'Title'],\n",
    "                'match_score': score\n",
    "            })\n",
    "\n",
    "# --- Step 4: Create and Analyze the Results DataFrame ---\n",
    "\n",
    "print(f\"\\nProcess complete. Found {len(citation_links)} potential links.\")\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "links_df = pd.DataFrame(citation_links)\n",
    "\n",
    "print(\"\\nSample of created links:\")\n",
    "print(links_df.head().to_string())\n",
    "\n",
    "# Analyze the distribution of match scores\n",
    "if not links_df.empty:\n",
    "    print(\"\\nDistribution of match scores:\")\n",
    "    print(links_df['match_score'].describe())\n",
    "    \n",
    "    # It's also very useful to look at the borderline cases\n",
    "    print(\"\\nExamples of borderline matches (score between 85 and 90):\")\n",
    "    borderline_cases = links_df[(links_df['match_score'] > 85) & (links_df['match_score'] < 90)]\n",
    "    print(borderline_cases.head(5).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda195f",
   "metadata": {},
   "source": [
    "## 3 Tryout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae43505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_title(title):\n",
    "    \"\"\"\n",
    "    Normalizes a paper title string, using a simple rule to detect spaced-out text.\n",
    "    Args:\n",
    "        title (str or any): The input title to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized title string. Returns an empty string if input is not a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "\n",
    "    if \"  \" in title:\n",
    "        placeholder = \"@@@\"\n",
    "        \n",
    "        title_fixed = re.sub(r'\\s{2,}', placeholder, title)\n",
    "        \n",
    "        title_fixed = title_fixed.replace(' ', '')\n",
    "        \n",
    "        title = title_fixed.replace(placeholder, ' ')\n",
    "        \n",
    "    title = title.lower()\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = \" \".join(title.split())\n",
    "    \n",
    "    return title\n",
    "\n",
    "def extract_year(date_string):\n",
    "    \"\"\"\n",
    "    Extracts the most likely publication year from a messy date string.\n",
    "    Args:\n",
    "        date_string (str): The messy string from the 'Date' column.\n",
    "\n",
    "    Returns:\n",
    "        int or None: The extracted four-digit year as an integer, or None if no\n",
    "                     plausible year can be found.\n",
    "    \"\"\"\n",
    "    if not isinstance(date_string, str):\n",
    "        return None\n",
    "\n",
    "    published_match = re.search(r'Published: .*?(\\b(19|20)\\d{2}\\b)', date_string)\n",
    "    if published_match:\n",
    "        return int(published_match.group(1))\n",
    "\n",
    "    all_years = re.findall(r'\\b(?:19|20)\\d{2}\\b', date_string)\n",
    "    \n",
    "    if all_years:\n",
    "        return int(all_years[-1])\n",
    "\n",
    "    # If no year could be found, return None\n",
    "    return \"no_date\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a6fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_pickle(\"citation_links_199142.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_pickle(\"DATASET/ED4RE_2503/ED4RE_2603.pickle\")\n",
    "is_list_mask = df['References'].apply(lambda x: isinstance(x, list))\n",
    "df_lists_only = df[is_list_mask]\n",
    "df = df_lists_only.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe701d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Normalizing Titles\")\n",
    "df['normalized_title'] = df['Title'].progress_apply(fix_title)\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting Years\")\n",
    "df['year'] = df['Date'].progress_apply(extract_year)\n",
    "\n",
    "\n",
    "# Let's use the high-confidence links for a cleaner graph\n",
    "CONFIDENCE_THRESHOLD = 90\n",
    "if 'match_score' in links.columns:\n",
    "    graph_df = links[links['match_score'] >= CONFIDENCE_THRESHOLD].copy()\n",
    "else:\n",
    "    graph_df = links.copy()\n",
    "\n",
    "print(f\"Building graph from {len(graph_df)} high-confidence links.\")\n",
    "\n",
    "# 1. Identify all unique nodes from your links\n",
    "# These are all the paper indices that are either a source or a target.\n",
    "all_node_indices = pd.concat([\n",
    "    graph_df['source_index'], \n",
    "    graph_df['target_index']\n",
    "]).unique()\n",
    "\n",
    "print(f\"Found {len(all_node_indices)} unique papers (nodes) in the network.\")\n",
    "\n",
    "# 2. Get the attributes for these nodes (the titles) from the main DataFrame\n",
    "# We select only the rows from the main 'df' that correspond to our nodes.\n",
    "node_attributes_df = df.loc[all_node_indices, ['normalized_title', 'year']]\n",
    "\n",
    "# 3. Create the list of nodes in the format networkx expects:\n",
    "# [(node_id, {attribute_dict}), (node_id_2, {attribute_dict_2}), ...]\n",
    "nodes_with_attrs = [\n",
    "    (index, {\"title\": row[\"normalized_title\"], \"year\": row[\"year\"]}) \n",
    "    for index, row in node_attributes_df.iterrows()\n",
    "]\n",
    "\n",
    "# 4. Create the list of edges from your links DataFrame\n",
    "# This is a simple list of (source, target) tuples.\n",
    "edges = list(graph_df[['source_index', 'target_index']].to_records(index=False))\n",
    "\n",
    "# 1. Create a new directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 2. Add the nodes and edges\n",
    "G.add_nodes_from(nodes_with_attrs)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print(\"\\nGraph created successfully!\")\n",
    "print(\"--- Basic Graph Statistics ---\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"Number of edges: {G.number_of_edges():,}\")\n",
    "\n",
    "# The density of a graph is the ratio of actual edges to all possible edges.\n",
    "# Citation networks are extremely sparse, so this number will be tiny.\n",
    "density = nx.density(G)\n",
    "print(f\"Graph density: {density:.6f}\")\n",
    "\n",
    "# --- Find the most influential papers (most cited) ---\n",
    "# In a citation network, this means finding the nodes with the highest \"in-degree\".\n",
    "in_degrees = G.in_degree() # This returns a list of (node, degree) tuples\n",
    "top_10_cited = sorted(in_degrees, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\n--- Top 10 Most Cited Papers in Your Corpus ---\")\n",
    "for node_id, degree in top_10_cited:\n",
    "    # Use the node attribute 'title' that we added earlier\n",
    "    title = G.nodes[node_id]['title']\n",
    "    print(f\"Citations: {degree:<5} | Title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, f\"citation_graph_{len(G.nodes)}.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec72bd",
   "metadata": {},
   "source": [
    "Overall, this fuzzy matching between titles and references is not working good, and is slow, doi with API request should be tackled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
